{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "import wolframalpha\n",
    "from groq import Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
    "groq_api_key=os.environ.get(\"GROQ_API_KEY\")\n",
    "model=\"llama-3.1-70b-versatile\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Groq(\n",
    "    api_key=groq_api_key,\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain the importance of language models\",\n",
    "        }\n",
    "    ],\n",
    "    model=model,\n",
    "    temperature=1,\n",
    "    max_tokens=1024,\n",
    "    top_p=1,\n",
    "    stream=True,\n",
    "    stop=None,\n",
    ")\n",
    "\n",
    "for chunk in chat_completion:\n",
    "    print(chunk.choices[0].delta.content or \"\", end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use model to create queries for Wolfram API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "wolfram_app_id = \"Y89ETU-L7Y6WK73H8\"\n",
    "os.environ[\"WOLFRAM_ALPHA_APPID\"] = wolfram_app_id\n",
    "client = wolframalpha.Client(wolfram_app_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wolfram_api_call(client, solve):\n",
    "    response = client.query(solve)\n",
    "    full_response = response.json()\n",
    "    pods = full_response[\"queryresult\"][\"pods\"][1][\"subpods\"]\n",
    "    return pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "prompt = '''Use Wolfram alpha api calls to solve this: {question}'''\n",
    "\n",
    "math_assitant_prompt = PromptTemplate(input_variables=[\"question\"],\n",
    "                                      template=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wolfram_response(verbose_query, app_id):\n",
    "    # Wolfram api fails with verbose query. RegEx to obtain query enclosed in quotes.\n",
    "    queries = re.findall(r'\"([^\"]*)\"', verbose_query)\n",
    "    query = queries[0]\n",
    "    params = {\n",
    "        \"input\": query,\n",
    "        \"appid\": app_id,\n",
    "        \"format\": \"plaintext\",\n",
    "        \"output\": \"json\",\n",
    "    }\n",
    "    response = requests.get(\"https://api.wolframalpha.com/v2/query\", params=params)\n",
    "    full_response = response.json()\n",
    "    pods = full_response[\"queryresult\"][\"pods\"][1][\"subpods\"]\n",
    "    return pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"input\": \"Solve integ 4x^2 dx\",\n",
    "    \"appid\": wolfram_app_id,\n",
    "    \"format\": \"plaintext\",\n",
    "    \"output\": \"json\",\n",
    "}\n",
    "response = requests.get(\"https://api.wolframalpha.com/v2/query\", params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Indefinite integral',\n",
       "  'scanner': 'Integral',\n",
       "  'id': 'IndefiniteIntegral',\n",
       "  'position': 100,\n",
       "  'error': False,\n",
       "  'numsubpods': 1,\n",
       "  'primary': True,\n",
       "  'subpods': [{'title': '',\n",
       "    'plaintext': 'integral4 x^2 dx = (4 x^3)/3 + constant'}],\n",
       "  'expressiontypes': {'name': 'Default'},\n",
       "  'states': [{'name': 'Step-by-step solution',\n",
       "    'input': 'IndefiniteIntegral__Step-by-step solution',\n",
       "    'stepbystep': True,\n",
       "    'buttonstyle': 'StepByStepSolution'}]},\n",
       " {'title': 'Plot of the integral',\n",
       "  'scanner': 'Integral',\n",
       "  'id': 'Plot',\n",
       "  'position': 200,\n",
       "  'error': False,\n",
       "  'numsubpods': 1,\n",
       "  'subpods': [{'title': '', 'plaintext': ''}],\n",
       "  'expressiontypes': {'name': 'Default'}}]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()[\"queryresult\"][\"pods\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Low-latency Large Language Models (LLMs) are crucial for various applications, particularly those requiring real-time or near-real-time interactions. The importance of low-latency LLMs can be understood from the following aspects:\\n\\n1. **Improved User Experience**: Low-latency LLMs enable faster response times, making interactions feel more natural and engaging. This is particularly important for applications like chatbots, virtual assistants, and language translation, where users expect quick and accurate responses.\\n\\n2. **Real-time Decision Making**: In applications like autonomous vehicles, medical diagnosis, or financial trading, low-latency LLMs can provide critical information in real-time, enabling timely decision-making and potentially saving lives or preventing financial losses.\\n\\n3. **Enhanced Productivity**: Low-latency LLMs can accelerate tasks like language translation, text summarization, and content generation, allowing professionals to work more efficiently and effectively. This can lead to increased productivity and better outcomes in fields like journalism, research, and marketing.\\n\\n4. **Competitive Advantage**: In industries where speed is a key differentiator, low-latency LLMs can provide a competitive edge. For example, in customer service, a company that can respond quickly and accurately to customer inquiries is more likely to build trust and loyalty.\\n\\n5. **Edge Computing and IoT Applications**: As IoT devices become increasingly prevalent, low-latency LLMs are essential for processing and analyzing data in real-time, enabling applications like smart homes, industrial automation, and smart cities.\\n\\n6. **Reduced Costs**: By reducing the time required for processing and responding to user inputs, low-latency LLMs can help minimize the costs associated with cloud computing, data storage, and network bandwidth.\\n\\nTo achieve low latency in LLMs, researchers and developers employ various techniques, including:\\n\\n1. **Model pruning and quantization**: Reducing the size and complexity of LLMs to improve inference speed.\\n2. **Knowledge distillation**: Transferring knowledge from large, pre-trained models to smaller, more efficient models.\\n3. **Specialized hardware**: Utilizing hardware accelerators like GPUs, TPUs, or ASICs to speed up LLM computations.\\n4. **Caching and memoization**: Storing frequently accessed information to reduce the need for repeated computations.\\n5. **Optimized software frameworks**: Leveraging optimized software frameworks and libraries to streamline LLM inference and processing.\\n\\nBy prioritizing low latency in LLMs, developers can create more responsive, efficient, and effective applications that meet the demands of real-time interactions and decision-making.', response_metadata={'token_usage': {'completion_tokens': 518, 'prompt_tokens': 52, 'total_tokens': 570, 'completion_time': 2.072, 'prompt_time': 0.012711284, 'queue_time': 0.004826345999999999, 'total_time': 2.084711284}, 'model_name': 'llama-3.1-70b-versatile', 'system_fingerprint': 'fp_b3ae7e594e', 'finish_reason': 'stop', 'logprobs': None}, id='run-330465a6-1142-4b06-a147-55179baa5b1d-0', usage_metadata={'input_tokens': 52, 'output_tokens': 518, 'total_tokens': 570})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatGroq(temperature=0, groq_api_key=groq_api_key, model=model)\n",
    "system = \"You are a helpful assistant.\"\n",
    "human = \"{text}\"\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)])\n",
    "\n",
    "chain = prompt | llm\n",
    "chain.invoke({\"text\": \"Explain the importance of low latency LLMs.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities.wolfram_alpha import WolframAlphaAPIWrapper\n",
    "wolfram = WolframAlphaAPIWrapper()\n",
    "# does not work in jupyter because jupyter is asyncio loop\n",
    "# wolfram.run(\"What is 2x+5 = -3x + 7?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def wolfram_call(query: str) -> str:\n",
    "    \"\"\"Calls the wolfram alpha api on query.\n",
    "\n",
    "    Args:\n",
    "        query (str): The query to use as a parameter in the function call. For example, \"Solve 3x=5\"\n",
    "\n",
    "    Returns:\n",
    "        str: Returns the result of the api call on the query.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"input\": query,\n",
    "        \"appid\": wolfram_app_id,\n",
    "        \"format\": \"plaintext\",\n",
    "        \"output\": \"json\",\n",
    "    }\n",
    "    response = requests.get(\"https://api.wolframalpha.com/v2/query\", params=params)\n",
    "    full_response = response.json()\n",
    "    pods = [x[\"subpods\"] for x in full_response[\"queryresult\"][\"pods\"]]\n",
    "    return str(pods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wolfram_call\n",
      "Calls the wolfram alpha api on query.\n",
      "\n",
      "    Args:\n",
      "        query (str): The query to use as a parameter in the function call. For example, \"Solve 3x=5\"\n",
      "\n",
      "    Returns:\n",
      "        str: Returns the result of the api call on the query.\n",
      "{'query': {'title': 'Query', 'type': 'string'}}\n"
     ]
    }
   ],
   "source": [
    "print(wolfram_call.name)\n",
    "print(wolfram_call.description)\n",
    "print(wolfram_call.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[[{'title': '', 'plaintext': 'integral4 x^2 dx = (4 x^3)/3 + constant'}], [{'title': '', 'plaintext': ''}]]\""
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wolfram_call.run(\"What is the integral of 4x^2?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_structured_output_runnable\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"\"\"You are a mathematical assistant. Please solve the questions in a step-by-step manner.\n",
    "         Use the wolfram tool wherever it is possible to create a valid query to solve any algebra required in the steps of the problem.\"\"\"),\n",
    "        MessagesPlaceholder(\"chat_history\", optional=True),\n",
    "        ('human', '{input}'),\n",
    "        MessagesPlaceholder(\"agent_scratchpad\"),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# structured_llm = create_structured_output_runnable(\n",
    "#     wolfram_call,\n",
    "#     llm,\n",
    "#     prompt,\n",
    "#     mode=\"openai-tools\",\n",
    "#     enforce_function_usage=True,\n",
    "#     return_single=True\n",
    "# )\n",
    "\n",
    "# structured_llm.invoke({\"input\": \"What is 3x^2 + 17x + 255 = 0?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
    "\n",
    "toolkit = [wolfram_call]\n",
    "\n",
    "agent = create_openai_tools_agent(llm, toolkit, prompt)\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=toolkit, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `wolfram_call` with `{'query': 'Solve x^2 + 2x + 5 = 0'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[[{'title': '', 'plaintext': 'solve x^2 + 2 x + 5 = 0'}], [{'title': '', 'plaintext': 'x = -1 - 2 i'}, {'title': '', 'plaintext': 'x = -1 + 2 i'}], [{'title': '', 'plaintext': ''}], [{'title': '', 'plaintext': '-2'}], [{'title': '', 'plaintext': '5'}]]\u001b[0m\u001b[32;1m\u001b[1;3mThe solutions to the equation x^2 + 2x + 5 = 0 are x = -1 - 2i and x = -1 + 2i.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "result = agent_executor.invoke({\"input\": \"Solve x^2 + 2x + 5 = 0\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `wolfram_call` with `{'query': '(r/n) * ((r-1)/(n-1)) = 1/2, solve for n and r, n and r are positive integers'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[[{'title': '', 'plaintext': 'solve r/n×(r - 1)/(n - 1) = 1/2 for n, r, n, r'}], [{'title': '', 'plaintext': 'n = 1/4 (-(1 + sqrt(2)) (3 - 2 sqrt(2))^(c_1) + (sqrt(2) - 1) (3 + 2 sqrt(2))^(c_1) + 2) and r = 1/8 ((2 + sqrt(2)) (3 - 2 sqrt(2))^(c_1) - (sqrt(2) - 2) (3 + 2 sqrt(2))^(c_1) + 4) and c_1>=2 and c_1 element Z'}], [{'title': '', 'plaintext': 'n = 4 and r = 3'}, {'title': '', 'plaintext': 'n = 21 and r = 15'}, {'title': '', 'plaintext': 'n = 120 and r = 85'}, {'title': '', 'plaintext': 'n = 697 and r = 493'}, {'title': '', 'plaintext': 'n = 4060 and r = 2871'}, {'title': '', 'plaintext': 'n = 23661 and r = 16731'}, {'title': '', 'plaintext': 'n = 137904 and r = 97513'}, {'title': '', 'plaintext': 'n = 803761 and r = 568345'}, {'title': '', 'plaintext': 'n = 4684660 and r = 3312555'}]]\u001b[0m\u001b[32;1m\u001b[1;3mThe smallest number of socks in the drawer can be 4, with 3 red socks and 1 black sock.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "result2 = agent_executor.invoke({\"input\": \"A drawer contains red socks and black socks. When two socks are drawn at random, the probability that both are red is 1/2. How small can the number of socks in the drawer be? \"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `wolfram_call` with `{'query': 'pi > 1'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[[{'title': '', 'plaintext': 'π>1'}], [{'title': '', 'primary': True, 'plaintext': 'True'}]]\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `wolfram_call` with `{'query': 'sum of 1/(n^pi) from n=1 to infinity'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[[{'title': '', 'plaintext': 'sum_(n=1)^∞ 1/n^π = ζ(π)≈1.1762'}], [{'title': '', 'plaintext': 'sum_(n=1)^∞ 1/n^π converges'}], [{'title': '', 'plaintext': 'sum_(n=1)^k 1/n^π = H_k^(π)'}], [{'title': '', 'plaintext': ''}], [{'title': '', 'plaintext': 'ζ(π) = sum_(k=1)^∞ k^(-π)'}, {'title': '', 'plaintext': 'ζ(π) = -(2^π sum_(k=1)^∞ (-1)^k k^(-π))/(-2 + 2^π)'}, {'title': '', 'plaintext': 'ζ(π) = (2^π sum_(k=0)^∞ (1 + 2 k)^(-π))/(-1 + 2^π)'}, {'title': '', 'plaintext': 'ζ(π) = e^( sum_(k=1)^∞ P(k π)/k)'}]]\u001b[0m\u001b[32;1m\u001b[1;3mThe series 1/(n^pi) converges. \n",
      "\n",
      "Step 1: We first need to determine if the series is a p-series. A p-series is a series of the form 1/n^p. In this case, p = pi. \n",
      "\n",
      "Step 2: We need to determine if p is greater than 1. If p is greater than 1, the series converges. If p is less than or equal to 1, the series diverges. \n",
      "\n",
      "Step 3: We use the wolfram_call function to determine if pi is greater than 1. The result is True, which means pi is greater than 1.\n",
      "\n",
      "Step 4: Since p is greater than 1, the series converges. We can use the wolfram_call function to find the sum of the series from n=1 to infinity. The result is approximately 1.1762.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "result3 = agent_executor.invoke({\"input\": \"Determine if the series of 1/(n^pi) converges or diverges. Explain each step.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
